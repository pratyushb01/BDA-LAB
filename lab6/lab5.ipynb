{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4744e33",
   "metadata": {},
   "source": [
    "## Lab No 6: Prediction with Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4229dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7256e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('DT').master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b989423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('../covtype.data',header=False,inferSchema=True) #header=False !!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e3ccce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: integer (nullable = true)\n",
      " |-- _c15: integer (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: integer (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: integer (nullable = true)\n",
      " |-- _c20: integer (nullable = true)\n",
      " |-- _c21: integer (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: integer (nullable = true)\n",
      " |-- _c24: integer (nullable = true)\n",
      " |-- _c25: integer (nullable = true)\n",
      " |-- _c26: integer (nullable = true)\n",
      " |-- _c27: integer (nullable = true)\n",
      " |-- _c28: integer (nullable = true)\n",
      " |-- _c29: integer (nullable = true)\n",
      " |-- _c30: integer (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: integer (nullable = true)\n",
      " |-- _c33: integer (nullable = true)\n",
      " |-- _c34: integer (nullable = true)\n",
      " |-- _c35: integer (nullable = true)\n",
      " |-- _c36: integer (nullable = true)\n",
      " |-- _c37: integer (nullable = true)\n",
      " |-- _c38: integer (nullable = true)\n",
      " |-- _c39: integer (nullable = true)\n",
      " |-- _c40: integer (nullable = true)\n",
      " |-- _c41: integer (nullable = true)\n",
      " |-- _c42: integer (nullable = true)\n",
      " |-- _c43: integer (nullable = true)\n",
      " |-- _c44: integer (nullable = true)\n",
      " |-- _c45: integer (nullable = true)\n",
      " |-- _c46: integer (nullable = true)\n",
      " |-- _c47: integer (nullable = true)\n",
      " |-- _c48: integer (nullable = true)\n",
      " |-- _c49: integer (nullable = true)\n",
      " |-- _c50: integer (nullable = true)\n",
      " |-- _c51: integer (nullable = true)\n",
      " |-- _c52: integer (nullable = true)\n",
      " |-- _c53: integer (nullable = true)\n",
      " |-- _c54: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1657e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames=['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Roadways','Hillshade_9am',\\\n",
    "        'Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points',]+[f'Wilderness_Area_{i+1}' for i in range(4)]+[f'Soil_Type_{i+1}' for i in range(40)]+['Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4ae5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.toDF(*colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eaadccab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "print(len(colNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ed96133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn('Cover_Type',F.col('Cover_Type').cast(T.DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0af83b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols=colNames[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dd683cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler=VectorAssembler(inputCols=input_cols,outputCol='featureVector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d647f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=DecisionTreeClassifier(labelCol='Cover_Type',\n",
    "                                  predictionCol='prediction',\n",
    "                                  featuresCol='featureVector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71a8b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline(stages=[assembler,classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d8ec2351",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid=ParamGridBuilder()\\\n",
    "    .addGrid(classifier.impurity,['gini','entropy'])\\\n",
    "    .addGrid(classifier.maxDepth,[10,20])\\\n",
    "    .addGrid(classifier.maxBins,[32,128])\\\n",
    "    .addGrid(classifier.minInfoGain,[0.01,0.1])\\\n",
    "    .addGrid(classifier.minInstancesPerNode,[1,10])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6b2389b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator=MulticlassClassificationEvaluator(\n",
    "    labelCol='Cover_Type', predictionCol='prediction',metricName='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a3902077",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval=CrossValidator(estimator=pipeline,estimatorParamMaps=paramGrid,evaluator=evaluator,numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2e695b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=df.randomSplit([0.9,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "490d7cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/10 16:19:34 WARN DAGScheduler: Broadcasting large task binary with size 1114.5 KiB\n",
      "25/09/10 16:19:34 WARN DAGScheduler: Broadcasting large task binary with size 1277.4 KiB\n",
      "25/09/10 16:19:34 WARN DAGScheduler: Broadcasting large task binary with size 1442.4 KiB\n",
      "25/09/10 16:19:34 WARN DAGScheduler: Broadcasting large task binary with size 1591.5 KiB\n",
      "25/09/10 16:19:35 WARN DAGScheduler: Broadcasting large task binary with size 1727.9 KiB\n",
      "25/09/10 16:19:35 WARN DAGScheduler: Broadcasting large task binary with size 1222.5 KiB\n",
      "25/09/10 16:19:37 WARN DAGScheduler: Broadcasting large task binary with size 1023.2 KiB\n",
      "25/09/10 16:19:37 WARN DAGScheduler: Broadcasting large task binary with size 1072.8 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5674.469s][warning][gc,alloc] Executor task launch worker for task 2.0 in stage 356.0 (TID 5579): Retried waiting for GCLocker too often allocating 10730 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/10 16:19:42 ERROR Executor: Exception in task 2.0 in stage 356.0 (TID 5579)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader.org$apache$spark$util$collection$ExternalSorter$SpillReader$$readNextItem(ExternalSorter.scala:556)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader$$anon$4.hasNext(ExternalSorter.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:139)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:382)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:373)\n",
      "\tat scala.collection.Iterator$$anon$1.next(Iterator.scala:145)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:421)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:406)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$5(ExternalSorter.scala:741)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5938/0x00007faa4110cc58.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4(ExternalSorter.scala:747)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4$adapted(ExternalSorter.scala:728)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5933/0x00007faa410f2000.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "25/09/10 16:19:42 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 356.0 (TID 5579),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader.org$apache$spark$util$collection$ExternalSorter$SpillReader$$readNextItem(ExternalSorter.scala:556)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader$$anon$4.hasNext(ExternalSorter.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:139)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:382)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:373)\n",
      "\tat scala.collection.Iterator$$anon$1.next(Iterator.scala:145)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:421)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:406)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$5(ExternalSorter.scala:741)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5938/0x00007faa4110cc58.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4(ExternalSorter.scala:747)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4$adapted(ExternalSorter.scala:728)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5933/0x00007faa410f2000.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "25/09/10 16:19:42 WARN TaskSetManager: Lost task 2.0 in stage 356.0 (TID 5579) (172.16.58.196 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader.org$apache$spark$util$collection$ExternalSorter$SpillReader$$readNextItem(ExternalSorter.scala:556)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader$$anon$4.hasNext(ExternalSorter.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:139)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:382)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:373)\n",
      "\tat scala.collection.Iterator$$anon$1.next(Iterator.scala:145)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:421)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:406)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$5(ExternalSorter.scala:741)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5938/0x00007faa4110cc58.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4(ExternalSorter.scala:747)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4$adapted(ExternalSorter.scala:728)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5933/0x00007faa410f2000.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\n",
      "25/09/10 16:19:42 ERROR TaskSetManager: Task 2 in stage 356.0 failed 1 times; aborting job\n",
      "25/09/10 16:19:42 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 356.0 failed 1 times, most recent failure: Lost task 2.0 in stage 356.0 (TID 5579) (172.16.58.196 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader.org$apache$spark$util$collection$ExternalSorter$SpillReader$$readNextItem(ExternalSorter.scala:556)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader$$anon$4.hasNext(ExternalSorter.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:139)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:382)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:373)\n",
      "\tat scala.collection.Iterator$$anon$1.next(Iterator.scala:145)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:421)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:406)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$5(ExternalSorter.scala:741)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5938/0x00007faa4110cc58.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4(ExternalSorter.scala:747)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4$adapted(ExternalSorter.scala:728)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5933/0x00007faa410f2000.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:740)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:739)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:665)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:210)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:304)\n",
      "\tat org.apache.spark.ml.classification.DecisionTreeClassifier.$anonfun$train$1(DecisionTreeClassifier.scala:143)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:116)\n",
      "\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:48)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:79)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader.org$apache$spark$util$collection$ExternalSorter$SpillReader$$readNextItem(ExternalSorter.scala:556)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader$$anon$4.hasNext(ExternalSorter.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:139)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:382)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:373)\n",
      "\tat scala.collection.Iterator$$anon$1.next(Iterator.scala:145)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:421)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:406)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$5(ExternalSorter.scala:741)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5938/0x00007faa4110cc58.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4(ExternalSorter.scala:747)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4$adapted(ExternalSorter.scala:728)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5933/0x00007faa410f2000.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\n",
      "25/09/10 16:19:42 WARN TaskSetManager: Lost task 5.0 in stage 356.0 (TID 5582) (172.16.58.196 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 356.0 failed 1 times, most recent failure: Lost task 2.0 in stage 356.0 (TID 5579) (172.16.58.196 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader.org$apache$spark$util$collection$ExternalSorter$SpillReader$$readNextItem(ExternalSorter.scala:556)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader$$anon$4.hasNext(ExternalSorter.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:139)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:382)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:373)\n",
      "\tat scala.collection.Iterator$$anon$1.next(Iterator.scala:145)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:421)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:406)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$5(ExternalSorter.scala:741)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5938/0x00007faa4110cc58.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4(ExternalSorter.scala:747)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4$adapted(ExternalSorter.scala:728)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5933/0x00007faa410f2000.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/10 16:19:42 WARN TaskSetManager: Lost task 10.0 in stage 356.0 (TID 5587) (172.16.58.196 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 356.0 failed 1 times, most recent failure: Lost task 2.0 in stage 356.0 (TID 5579) (172.16.58.196 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader.org$apache$spark$util$collection$ExternalSorter$SpillReader$$readNextItem(ExternalSorter.scala:556)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader$$anon$4.hasNext(ExternalSorter.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:139)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:382)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:373)\n",
      "\tat scala.collection.Iterator$$anon$1.next(Iterator.scala:145)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:421)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:406)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$5(ExternalSorter.scala:741)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5938/0x00007faa4110cc58.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4(ExternalSorter.scala:747)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4$adapted(ExternalSorter.scala:728)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5933/0x00007faa410f2000.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/10 16:19:42 WARN TaskSetManager: Lost task 3.0 in stage 356.0 (TID 5580) (172.16.58.196 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 356.0 failed 1 times, most recent failure: Lost task 2.0 in stage 356.0 (TID 5579) (172.16.58.196 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader.org$apache$spark$util$collection$ExternalSorter$SpillReader$$readNextItem(ExternalSorter.scala:556)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader$$anon$4.hasNext(ExternalSorter.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:139)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:382)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:373)\n",
      "\tat scala.collection.Iterator$$anon$1.next(Iterator.scala:145)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:421)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:406)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$5(ExternalSorter.scala:741)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5938/0x00007faa4110cc58.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4(ExternalSorter.scala:747)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4$adapted(ExternalSorter.scala:728)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5933/0x00007faa410f2000.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/10 16:19:42 WARN TaskSetManager: Lost task 12.0 in stage 356.0 (TID 5589) (172.16.58.196 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 356.0 failed 1 times, most recent failure: Lost task 2.0 in stage 356.0 (TID 5579) (172.16.58.196 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2121)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1721)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader.org$apache$spark$util$collection$ExternalSorter$SpillReader$$readNextItem(ExternalSorter.scala:556)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$SpillReader$$anon$4.hasNext(ExternalSorter.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:139)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:382)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$1.next(ExternalSorter.scala:373)\n",
      "\tat scala.collection.Iterator$$anon$1.next(Iterator.scala:145)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:421)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$anon$2.next(ExternalSorter.scala:406)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$5(ExternalSorter.scala:741)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5938/0x00007faa4110cc58.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4(ExternalSorter.scala:747)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$4$adapted(ExternalSorter.scala:728)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter$$Lambda$5933/0x00007faa410f2000.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/09/10 16:19:42 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$5952/0x00007faa40eb9e90@56517fb9 rejected from java.util.concurrent.ThreadPoolExecutor@16132a53[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 5591]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:140)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:813)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:786)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:73)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m crossModel=\u001b[43mcrossval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/ml/base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/ml/tuning.py:858\u001b[39m, in \u001b[36mCrossValidator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    854\u001b[39m tasks = \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m    855\u001b[39m     inheritable_thread_target(dataset.sparkSession),\n\u001b[32m    856\u001b[39m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[32m    857\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/multiprocessing/pool.py:873\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    872\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/multiprocessing/pool.py:125\u001b[39m, in \u001b[36mworker\u001b[39m\u001b[34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result = (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/ml/tuning.py:858\u001b[39m, in \u001b[36mCrossValidator._fit.<locals>.<lambda>\u001b[39m\u001b[34m(f)\u001b[39m\n\u001b[32m    854\u001b[39m tasks = \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m    855\u001b[39m     inheritable_thread_target(dataset.sparkSession),\n\u001b[32m    856\u001b[39m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[32m    857\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool.imap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[32m    859\u001b[39m     metrics_all[i][j] = metric\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/util.py:435\u001b[39m, in \u001b[36minheritable_thread_target.<locals>.outer.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m     session.addTag(tag)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/ml/tuning.py:115\u001b[39m, in \u001b[36m_parallelFitTasks.<locals>.singleTask\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msingleTask\u001b[39m() -> Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     index, model = \u001b[38;5;28mnext\u001b[39m(modelIter)\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/ml/base.py:96\u001b[39m, in \u001b[36m_FitMultipleIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mself\u001b[39m.counter += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/ml/base.py:154\u001b[39m, in \u001b[36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[39m\u001b[34m(index)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) -> M:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/ml/base.py:201\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/ml/pipeline.py:138\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     model = \u001b[43mstage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     transformers.append(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/ml/base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/ml/util.py:164\u001b[39m, in \u001b[36mtry_remote_fit.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/ml/wrapper.py:411\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/ml/wrapper.py:407\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31m<class 'str'>\u001b[39m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2205\u001b[39m, in \u001b[36mInteractiveShell.showtraceback\u001b[39m\u001b[34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[39m\n\u001b[32m   2202\u001b[39m         traceback.print_exc()\n\u001b[32m   2203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2205\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_pdb:\n\u001b[32m   2207\u001b[39m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[32m   2208\u001b[39m     \u001b[38;5;28mself\u001b[39m.debugger(force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/ipykernel/zmqshell.py:559\u001b[39m, in \u001b[36mZMQInteractiveShell._showtraceback\u001b[39m\u001b[34m(self, etype, evalue, stb)\u001b[39m\n\u001b[32m    553\u001b[39m sys.stdout.flush()\n\u001b[32m    554\u001b[39m sys.stderr.flush()\n\u001b[32m    556\u001b[39m exc_content = {\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype.\u001b[34m__name__\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(evalue),\n\u001b[32m    560\u001b[39m }\n\u001b[32m    562\u001b[39m dh = \u001b[38;5;28mself\u001b[39m.displayhook\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/py4j/protocol.py:472\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    471\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/IPython/core/async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3413\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3409\u001b[39m exec_count = \u001b[38;5;28mself\u001b[39m.execution_count\n\u001b[32m   3410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error_in_exec:\n\u001b[32m   3411\u001b[39m     \u001b[38;5;66;03m# Store formatted traceback and error details\u001b[39;00m\n\u001b[32m   3412\u001b[39m     \u001b[38;5;28mself\u001b[39m.history_manager.exceptions[exec_count] = (\n\u001b[32m-> \u001b[39m\u001b[32m3413\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_exception_for_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror_in_exec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3414\u001b[39m     )\n\u001b[32m   3416\u001b[39m \u001b[38;5;66;03m# Each cell is a *single* input, regardless of how many lines it has\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[38;5;28mself\u001b[39m.execution_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3474\u001b[39m, in \u001b[36mInteractiveShell._format_exception_for_storage\u001b[39m\u001b[34m(self, exception, filename, running_compiled_code)\u001b[39m\n\u001b[32m   3470\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   3471\u001b[39m             \u001b[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001b[39;00m\n\u001b[32m   3472\u001b[39m             stb = traceback.format_exception(etype, evalue, tb)\n\u001b[32m-> \u001b[39m\u001b[32m3474\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: etype.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(evalue), \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/py4j/protocol.py:472\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    471\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "crossModel=crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ebf589",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=cvModel.transform(test)\n",
    "accuracy=evaluator.evaluate(predictions)\n",
    "print(\"Accuracy Output:\",accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd2250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can check which was the best model\n",
    "bestModel=cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix=predictions.groupBy('Cover_Type').pivot('prediction',range(1,9)).count().na.fill(0.0).orderBy('Cover_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163748a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
